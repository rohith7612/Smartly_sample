import os
import logging
import PyPDF2
import docx
import pytesseract
from PIL import Image, ImageOps, ImageFilter
import openai
import os
from django.conf import settings
try:
    # Optional providers; import lazily so app works without them
    from anthropic import Anthropic
except Exception:
    Anthropic = None
try:
    import google.generativeai as genai
except Exception:
    genai = None
try:
    from huggingface_hub import InferenceClient
except Exception:
    InferenceClient = None
from django.conf import settings
from youtube_transcript_api import YouTubeTranscriptApi
import re
import json
from urllib import request as urlrequest
from urllib.error import URLError, HTTPError
from urllib.parse import urlencode, urlparse, parse_qs, quote, unquote

# Configure OpenAI API key
openai.api_key = os.getenv('OPENAI_API_KEY') or getattr(settings, 'OPENAI_API_KEY', None)
ANTHROPIC_API_KEY = os.getenv('ANTHROPIC_API_KEY') or getattr(settings, 'ANTHROPIC_API_KEY', None)
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY') or getattr(settings, 'GOOGLE_API_KEY', None)
HF_API_KEY = os.getenv('HUGGINGFACE_API_KEY') or getattr(settings, 'HUGGINGFACE_API_KEY', None)

# Configure Tesseract OCR binary path if provided via env or settings
_tesseract_cmd = os.getenv('TESSERACT_CMD') or getattr(settings, 'TESSERACT_CMD', None)
if _tesseract_cmd:
    try:
        pytesseract.pytesseract.tesseract_cmd = _tesseract_cmd
    except Exception:
        # Silently ignore misconfig; we'll surface a clearer error during use
        pass

def extract_text_from_pdf(file_path):
    """Extract text from PDF file with memory optimization"""
    text_parts = []
    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        for page_num in range(len(pdf_reader.pages)):
            page_text = pdf_reader.pages[page_num].extract_text()
            if page_text:
                text_parts.append(page_text)
    return ''.join(text_parts)

def extract_text_from_docx(file_path):
    """Extract text from DOCX file with memory optimization"""
    doc = docx.Document(file_path)
    text_parts = []
    for paragraph in doc.paragraphs:
        if paragraph.text:
            text_parts.append(paragraph.text)
    return '\n'.join(text_parts)

def extract_text_from_image(file_path):
    """Extract text from image using OCR with light preprocessing.
    - Supports JPG/PNG and similar raster formats.
    - Applies grayscale + contrast + slight sharpening to improve OCR.
    - Uses page segmentation mode suitable for blocks of text.
    """
    try:
        image = Image.open(file_path)
        # Convert to grayscale and lightly enhance contrast
        img = ImageOps.grayscale(image)
        img = img.filter(ImageFilter.UnsharpMask(radius=1.2, percent=150, threshold=3))
        # Optional light normalization for very dark or bright scans
        img = ImageOps.autocontrast(img, cutoff=2)

        # Use an OCR configuration tuned for uniform text blocks
        ocr_config = "--psm 6"
        text = pytesseract.image_to_string(img, config=ocr_config)
        return text.strip()
    except pytesseract.TesseractNotFoundError:
        return (
            "OCR not available: Tesseract binary not found. "
            "Install Tesseract OCR and set TESSERACT_CMD to its path (e.g., "
            "C:\\Program Files\\Tesseract-OCR\\tesseract.exe on Windows)."
        )
    except Exception as e:
        return f"Error extracting text from image: {str(e)}"

def extract_text_from_file(file_path, file_type):
    """Extract text from file based on file type with memory optimization"""
    if file_type == 'pdf':
        return extract_text_from_pdf(file_path)
    elif file_type == 'docx':
        return extract_text_from_docx(file_path)
    elif file_type == 'image':
        return extract_text_from_image(file_path)
    elif file_type == 'txt':
        # Read text files in chunks for memory efficiency
        text_parts = []
        with open(file_path, 'r', encoding='utf-8') as file:
            while True:
                chunk = file.read(8192)  # Read 8KB at a time
                if not chunk:
                    break
                text_parts.append(chunk)
        return ''.join(text_parts)
    else:
        return "Unsupported file type"

def get_youtube_video_id(url):
    """Extract YouTube video ID from URL"""
    youtube_regex = r'(?:youtube\.com\/(?:[^\/\n\s]+\/\S+\/|(?:v|e(?:mbed)?)\/|\S*?[?&]v=)|youtu\.be\/)([a-zA-Z0-9_-]{11})'
    match = re.search(youtube_regex, url)
    return match.group(1) if match else None

def get_youtube_transcript(video_id):
    """Get transcript from YouTube video"""
    try:
        # Create an instance of the API and use the fetch method
        transcript_api = YouTubeTranscriptApi()
        fetched_transcript = transcript_api.fetch(video_id)
        # Convert to raw data and extract text
        transcript = ' '.join([item['text'] for item in fetched_transcript.to_raw_data()])
        return transcript
    except Exception as e:
        return f"Error getting transcript: {str(e)}"

def _route_chat(messages, system_prompt=None, model="gpt-3.5-turbo", max_tokens=800):
    """Route chat to the appropriate provider based on model string.
    Resolve API keys at call time to avoid import-order issues.
    """
    def _strip_think_blocks(text):
        try:
            if not isinstance(text, str):
                return text
            # Remove <think>...</think> blocks
            stripped = re.sub(r"<\s*think\b[^>]*>[\s\S]*?<\s*/\s*think\s*>", "", text, flags=re.IGNORECASE)
            if stripped.strip():
                return stripped
            # If stripping leaves nothing, extract inner <think> content and return it without tags
            inner_parts = re.findall(r"<\s*think\b[^>]*>([\s\S]*?)<\s*/\s*think\s*>", text, flags=re.IGNORECASE)
            joined = "\n\n".join([p.strip() for p in inner_parts if isinstance(p, str) and p.strip()])
            return joined if joined.strip() else text
        except Exception:
            return text
    try:
        m = (model or "gpt-3.5-turbo").lower()
        ALLOW_FALLBACKS = bool(getattr(settings, 'ALLOW_MODEL_FALLBACKS', False))
        # Resolve keys dynamically each call
        OPENAI_KEY = os.getenv('OPENAI_API_KEY') or getattr(settings, 'OPENAI_API_KEY', None)
        ANTH_KEY = os.getenv('ANTHROPIC_API_KEY') or getattr(settings, 'ANTHROPIC_API_KEY', None)
        GOOGLE_KEY = os.getenv('GOOGLE_API_KEY') or getattr(settings, 'GOOGLE_API_KEY', None)
        HF_KEY = os.getenv('HUGGINGFACE_API_KEY') or getattr(settings, 'HUGGINGFACE_API_KEY', None)
        logging.getLogger(__name__).debug(
            f"Routing chat: model='{model}', fallbacks={'on' if ALLOW_FALLBACKS else 'off'}, "
            f"keys={{'openai':{bool(OPENAI_KEY)}, 'anthropic':{bool(ANTH_KEY)}, 'google':{bool(GOOGLE_KEY)}, 'hf':{bool(HF_KEY)}}}"
        )
        # Consolidate effective system instruction: persona + any system messages (e.g., doc context)
        sys_msgs = [
            (msg.get('content') or '').strip()
            for msg in (messages or []) if msg.get('role') == 'system'
        ]
        effective_system = (system_prompt or '').strip()
        if sys_msgs:
            extra = "\n\n".join([s for s in sys_msgs if s])
            effective_system = (effective_system + ("\n\n" + extra if effective_system else extra)).strip()
        # Use only the latest user message for single-turn providers to reduce drift
        last_user_msg = ''
        for msg in reversed(messages or []):
            if msg.get('role') == 'user':
                last_user_msg = (msg.get('content') or '').strip()
                break
        if m.startswith("claude") or m.startswith("anthropic"):
            if Anthropic is None or not ANTH_KEY:
                logging.getLogger(__name__).warning(
                    f"Anthropic not configured: import={'ok' if Anthropic else 'missing'}, key_present={bool(ANTH_KEY)}"
                )
                if not ALLOW_FALLBACKS:
                    return f"Provider not configured for model '{model}'. Set ANTHROPIC_API_KEY to use this model.\n\n[model: {model}]"
                # Provider not configured: fall back to OpenAI only if allowed
                final_messages = []
                if system_prompt:
                    final_messages.append({"role": "system", "content": system_prompt})
                final_messages.extend(messages)
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=final_messages,
                    max_tokens=max_tokens,
                    temperature=0.3,
                )
                return (response.choices[0].message.content or "") + "\n\n[model: gpt-3.5-turbo]"
            client = Anthropic(api_key=ANTH_KEY)
            # Anthropic: pass effective system instruction and latest user turn
            user_content = [{"type": "text", "text": last_user_msg or ''}]
            try:
                resp = client.messages.create(
                    model=model,
                    max_tokens=max_tokens,
                    system=effective_system or None,
                    messages=[{"role": "user", "content": user_content}]
                )
            except Exception as e:
                logging.getLogger(__name__).exception(f"Anthropic call failed for model='{model}': {e}")
                return f"Error calling Anthropic model '{model}'.\n\n[model: {model}]"
            out = []
            for p in getattr(resp, 'content', []):
                try:
                    if getattr(p, 'type', '') == 'text':
                        out.append(p.text)
                except Exception:
                    pass
            logging.getLogger(__name__).debug(
                f"Anthropic success: model='{model}', tokens={max_tokens}, reply_len={len(''.join(out))}"
            )
            return (("".join(out) or "") + f"\n\n[model: {model}]")
        elif m.startswith("gemini") or m.startswith("google"):
            if genai is None or not GOOGLE_KEY:
                logging.getLogger(__name__).warning(
                    f"Gemini not configured: import={'ok' if genai else 'missing'}, key_present={bool(GOOGLE_KEY)}"
                )
                # Provider not configured: gracefully fall back to OpenAI
                final_messages = []
                if system_prompt:
                    final_messages.append({"role": "system", "content": system_prompt})
                final_messages.extend(messages)
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=final_messages,
                    max_tokens=max_tokens,
                    temperature=0.3,
                )
                return response.choices[0].message.content
            genai.configure(api_key=GOOGLE_KEY)
            # Normalize common aliases to current API names
            alias = (model or "gemini-2.5-flash").lower()
            if alias.endswith("-flash-latest"):
                alias = alias.replace("-flash-latest", "-flash")
            if alias.endswith("-pro-latest"):
                alias = alias.replace("-pro-latest", "-pro")
            # Map legacy 1.5 ids to 2.5 when requested keys don't allow 1.5
            if alias in ("gemini-1.5-flash", "gemini-1.5-pro"):
                alias = alias.replace("1.5", "2.5")
            # Build model client with system instruction
            mdl = genai.GenerativeModel(alias, system_instruction=effective_system or None)
            try:
                # Single-turn with the latest user question; system instruction carries context
                prompt_in = last_user_msg or ""
                resp = mdl.generate_content(prompt_in, generation_config={"max_output_tokens": max_tokens})
                logging.getLogger(__name__).debug(
                    f"Gemini success: alias='{alias}', reply_len={len(getattr(resp, 'text', '') or '')}"
                )
                return ((getattr(resp, 'text', '') or "") + f"\n\n[model: {alias}]")
            except Exception as e:
                # If the selected model is unavailable for this key or method,
                # try alternate Gemini aliases within provider before erroring.
                logging.getLogger(__name__).warning(
                    f"Gemini generate_content failed for alias='{alias}': {e}"
                )
                try:
                    available = list(getattr(genai, 'list_models')())
                    # Prefer matching family (flash/pro) with generateContent support
                    family = 'flash' if 'flash' in alias else ('pro' if 'pro' in alias else '')
                    fallback_name = None
                    for mdef in available:
                        name = getattr(mdef, 'name', '')
                        methods = set(getattr(mdef, 'supported_generation_methods', []) or [])
                        if ('generateContent' in methods) and name.startswith('models/'):
                            base = name.split('/')[-1]
                            if family and family in base:
                                fallback_name = base
                                break
                            if not fallback_name:
                                fallback_name = base
                    if fallback_name:
                        mdl2 = genai.GenerativeModel(fallback_name, system_instruction=effective_system or None)
                        resp2 = mdl2.generate_content(prompt_in, generation_config={"max_output_tokens": max_tokens})
                        logging.getLogger(__name__).debug(
                            f"Gemini fallback success: picked='{fallback_name}', reply_len={len(getattr(resp2, 'text', '') or '')}"
                        )
                        out = getattr(resp2, 'text', '') or ""
                        out = _strip_think_blocks(out)
                        return (out + f"\n\n[model: {fallback_name}]")
                except Exception:
                    logging.getLogger(__name__).error(
                        f"Gemini fallback resolution failed for alias='{alias}'"
                    )
                    return f"Selected model '{alias}' is unavailable for the current API key or method.\n\n[model: {alias}]"
        elif ('/' in (model or '')) or m.startswith('hf') or ('huggingface' in m):
            # Hugging Face Inference API
            if InferenceClient is None or not HF_KEY:
                logging.getLogger(__name__).warning(
                    f"HuggingFace not configured: import={'ok' if InferenceClient else 'missing'}, key_present={bool(HF_KEY)}"
                )
                if not ALLOW_FALLBACKS:
                    return f"Provider not configured for Hugging Face model '{model}'. Set HUGGINGFACE_API_KEY to use this model.\n\n[model: {model}]"
                # Provider not configured: fallback to OpenAI only if allowed
                final_messages = []
                if effective_system:
                    final_messages.append({"role": "system", "content": effective_system})
                final_messages.extend(messages)
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=final_messages,
                    max_tokens=max_tokens,
                    temperature=0.3,
                )
                return (response.choices[0].message.content or "") + "\n\n[model: gpt-3.5-turbo]"
            # Build chat messages for OpenAI-compatible chat completions
            final_messages = []
            if effective_system:
                final_messages.append({"role": "system", "content": effective_system})
            final_messages.extend(messages)
            try:
                # Normalize model/provider for Inference Providers router
                raw_model = (model or '').strip()
                hf_model = raw_model
                provider = 'hf-inference'
                # If model is of form "org/repo:suffix", detect provider/policy vs revision
                if ':' in raw_model and '/' in raw_model:
                    base, suffix = raw_model.split(':', 1)
                    low = suffix.strip().lower()
                    provider_aliases = {
                        'hf': 'hf-inference',
                        'hf-inference': 'hf-inference',
                        'novita': 'novita',
                        'groq': 'groq',
                        'together': 'together',
                        'fireworks': 'fireworks-ai',
                        'fireworks-ai': 'fireworks-ai',
                        'fal': 'fal-ai',
                        'fal-ai': 'fal-ai',
                        'sambanova': 'sambanova',
                        'cohere': 'cohere',
                        'replicate': 'replicate',
                        'fastest': 'auto',
                        'cheapest': 'auto',
                    }
                    if low in provider_aliases:
                        provider = provider_aliases[low]
                        hf_model = base
                    else:
                        # Treat suffix as Hub revision when not a known provider/policy
                        hf_model = f"{base}@{suffix}"
                # Also support explicit provider via "org/repo@provider"
                if '@' in hf_model and '/' in hf_model:
                    base, alias = hf_model.split('@', 1)
                    hf_model = base
                    alias = alias.strip().lower()
                    provider_map = {
                        'hf': 'hf-inference',
                        'hf-inference': 'hf-inference',
                        'novita': 'novita',
                        'groq': 'groq',
                        'together': 'together',
                        'fireworks-ai': 'fireworks-ai',
                        'fal-ai': 'fal-ai',
                        'sambanova': 'sambanova',
                        'cohere': 'cohere',
                        'replicate': 'replicate',
                    }
                    provider = provider_map.get(alias, provider)
                client = InferenceClient(
                    provider=provider,
                    api_key=HF_KEY,
                )
                logging.getLogger(__name__).debug(
                    f"HuggingFace routing: base_model='{hf_model}', provider='{provider}'"
                )
                # Call OpenAI-compatible chat completions on the router
                completion = client.chat.completions.create(
                    model=hf_model,
                    messages=final_messages,
                    max_tokens=max_tokens,
                    temperature=0.3,
                )
                try:
                    # Log response shape for debugging (truncated)
                    snap = None
                    if hasattr(completion, 'model_dump_json'):
                        snap = completion.model_dump_json()[:600]
                    elif hasattr(completion, 'dict'):
                        snap = str(completion.dict())[:600]
                    else:
                        snap = str(completion)[:600]
                    logging.getLogger(__name__).debug(
                        f"HuggingFace completion snapshot (trunc): {snap}"
                    )
                except Exception:
                    pass
                # Extract content defensively
                content = None
                try:
                    if completion and getattr(completion, 'choices', None):
                        choice0 = completion.choices[0]
                        message = getattr(choice0, 'message', None) or (
                            choice0.get('message') if isinstance(choice0, dict) else None
                        )
                        content = getattr(message, 'content', None) if message is not None else None
                        if content is None and isinstance(message, dict):
                            content = message.get('content')
                        # If content is a list of segments (OpenAI Responses-style), join text parts
                        if isinstance(content, list):
                            try:
                                # Prefer explicit 'output_text' segments when present
                                has_output = any(isinstance(seg, dict) and seg.get('type') == 'output_text' for seg in content)
                                segments = []
                                for seg in content:
                                    if isinstance(seg, dict):
                                        if has_output and seg.get('type') != 'output_text':
                                            continue
                                        txt = seg.get('text') or seg.get('content') or seg.get('value')
                                        if isinstance(txt, str) and txt:
                                            segments.append(txt)
                                    elif isinstance(seg, str):
                                        if not has_output and seg:
                                            segments.append(seg)
                                content = "\n".join(segments).strip()
                            except Exception:
                                # Fall through to other extraction paths
                                pass
                        # Fallback: some routers may place text at the choice level
                        if not content:
                            content = getattr(choice0, 'text', None) if hasattr(choice0, 'text') else (
                                choice0.get('text') if isinstance(choice0, dict) else None
                            )
                        # Final fallback: attempt to flatten any nested message/content structures
                        if not content:
                            try:
                                as_dict = None
                                if hasattr(completion, 'model_dump'):
                                    as_dict = completion.model_dump()
                                elif hasattr(completion, 'dict'):
                                    as_dict = completion.dict()
                                elif isinstance(completion, dict):
                                    as_dict = completion
                                if isinstance(as_dict, dict):
                                    choices = as_dict.get('choices') or []
                                    if choices:
                                        m = choices[0].get('message') if isinstance(choices[0], dict) else None
                                        cont = None
                                        if isinstance(m, dict):
                                            cont = m.get('content')
                                        if isinstance(cont, list):
                                            parts = []
                                            for seg in cont:
                                                txt = None
                                                if isinstance(seg, dict):
                                                    txt = seg.get('text') or seg.get('content') or seg.get('value')
                                                elif isinstance(seg, str):
                                                    txt = seg
                                                if isinstance(txt, str) and txt:
                                                    parts.append(txt)
                                            content = "\n".join(parts).strip()
                                        elif isinstance(cont, str):
                                            content = cont
                            except Exception:
                                pass
                except Exception:
                    content = None
                text_out = content or ''
                text_out = _strip_think_blocks(text_out)
                logging.getLogger(__name__).debug(
                    f"HuggingFace success: model='{hf_model}', provider='{provider}', reply_len={len(text_out)}"
                )
                return text_out + f"\n\n[model: {hf_model}:{provider}]"
            except Exception:
                logging.getLogger(__name__).exception(
                    f"HuggingFace call failed for model='{model}'"
                )
                if not ALLOW_FALLBACKS:
                    return f"Error calling Hugging Face model '{model}'.\n\n[model: {model}]"
                # Fallback to OpenAI on error only if allowed
                final_messages = []
                if effective_system:
                    final_messages.append({"role": "system", "content": effective_system})
                final_messages.extend(messages)
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=final_messages,
                    max_tokens=max_tokens,
                    temperature=0.3,
                )
                return (response.choices[0].message.content or "") + "\n\n[model: gpt-3.5-turbo]"
        else:
            # Default OpenAI
            final_messages = []
            if system_prompt:
                final_messages.append({"role": "system", "content": system_prompt})
            final_messages.extend(messages)
            response = openai.ChatCompletion.create(
                model=model or "gpt-3.5-turbo",
                messages=final_messages,
                max_tokens=max_tokens,
                temperature=0.3,
            )
            out = response.choices[0].message.content or ""
            out = _strip_think_blocks(out)
            return (out + f"\n\n[model: {model or 'gpt-3.5-turbo'}]")
    except Exception as e:
        return f"Error chatting: {str(e)}\n\n[model: {model or 'unknown'}]"

def summarize_text(text, target_words=None, max_tokens=500, preset=None, model=None):
    """Summarize text using selected model/provider with optional preset formatting."""
    try:
        word_instruction = "" if not target_words else f" in approximately {int(target_words)} words"
        preset_instruction = ""
        if preset == 'bullet_points':
            preset_instruction = "Format strictly as a markdown bullet list. Use '-' at the start of each line. No introduction or conclusion. Keep bullets concise and study-friendly."
        elif preset == 'detailed_summary':
            preset_instruction = " Provide a comprehensive paragraph-style summary."
        elif preset == 'study_notes':
            preset_instruction = " Produce study notes: headings for topics, sub-bullets for key concepts and definitions."
        elif preset == 'brief_summary':
            preset_instruction = " Keep it brief for quick revision."
        messages = [
            {"role": "user", "content": f"Summarize the following text{word_instruction} and {preset_instruction} Avoid omitting key points.\n\n{text}"}
        ]
        return _route_chat(messages, system_prompt="You are a helpful assistant that summarizes text clearly and faithfully.", model=model or "gpt-3.5-turbo", max_tokens=max_tokens)
    except Exception as e:
        return f"Error summarizing text: {str(e)}"

def generate_answers(text, target_words=None, max_tokens=500, preset=None, model=None):
    """Generate answers using selected model/provider with optional preset type."""
    try:
        word_instruction = "" if not target_words else f" in approximately {int(target_words)} words"
        preset_instruction = ""
        if preset == 'exam_answers':
            preset_instruction = " Generate comprehensive, step-by-step exam answers. Use numbered steps and short headings for clarity."
        elif preset == 'practice_questions':
            preset_instruction = " Create 6-10 practice questions with detailed answers. Format as a numbered list where each item contains 'Q:' followed by the question and 'A:' followed by the answer."
        elif preset == 'study_plan':
            preset_instruction = " Draft a personalized study schedule. Use a bullet list grouped by days/weeks with time blocks and goals."
        messages = [
            {"role": "user", "content": f"Generate clear, step-by-step answers{word_instruction} to the following questions or content.{preset_instruction}\n\n{text}"}
        ]
        return _route_chat(messages, system_prompt="You are a helpful assistant that generates accurate, well-structured answers.", model=model or "gpt-3.5-turbo", max_tokens=max_tokens)
    except Exception as e:
        return f"Error generating answers: {str(e)}"

def analyze_text(text, target_words=None, max_tokens=500, preset=None, model=None):
    """Analyze text using selected model/provider with optional preset for analysis type."""
    try:
        word_instruction = "" if not target_words else f" in approximately {int(target_words)} words"
        preset_instruction = ""
        if preset == 'question_patterns':
            preset_instruction = " Identify recurring question patterns and topics. Output a bullet list. For each pattern, include a short label and 1-2 example phrasings."
        elif preset == 'predict_questions':
            preset_instruction = " Predict likely exam questions based on the content. Output as a numbered list of questions only, optionally include one-sentence rationale per item."
        elif preset == 'topic_importance':
            preset_instruction = " Rank topics by exam importance as a numbered list from most to least important, with a brief justification for each."
        messages = [
            {"role": "user", "content": f"Analyze the following text, identify key insights, and rank topics by importance{word_instruction}.{preset_instruction}\n\n{text}"}
        ]
        return _route_chat(messages, system_prompt="You are a helpful assistant that analyzes text and ranks topics by importance.", model=model or "gpt-3.5-turbo", max_tokens=max_tokens)
    except Exception as e:
        return f"Error analyzing text: {str(e)}"

def translate_text(text, target_language, source_language='auto', max_tokens=500, model=None):
    """Translate text using selected model/provider."""
    try:
        messages = [
            {"role": "user", "content": f"Please translate the following text from {source_language} to {target_language}:\n\n{text}"}
        ]
        return _route_chat(messages, system_prompt="You are a helpful assistant that translates text.", model=model or "gpt-3.5-turbo", max_tokens=max_tokens)
    except Exception as e:
        return f"Error translating text: {str(e)}"

def translate_text_free(text, target_language_code, source_language_code='auto'):
    """Translate text using the free LibreTranslate API.
    - Tries multiple public endpoints for resilience.
    - Falls back to form-encoded POST if JSON fails.
    - Splits large inputs into chunks to avoid payload limits.
    - Does not require any API key.
    - target_language_code: e.g., 'es', 'fr', 'de', 'hi', 'en'.
    """
    endpoints = [
        'https://libretranslate.de/translate',
        'https://translate.argosopentech.com/translate',
        'https://translate.astian.org/translate',
        'https://libretranslate.com/translate',
    ]

    def _detect_language(sample_text):
        """Detect language using LibreTranslate /detect endpoint across fallbacks.
        Returns a language code or None if detection fails.
        """
        sample = (sample_text or '').strip()
        if not sample:
            return None
        sample = sample[:1000]
        detect_paths = [e.replace('/translate', '/detect') for e in endpoints]
        payload = {'q': sample}
        # Try JSON then form-encoded for each endpoint
        for endpoint in detect_paths:
            try:
                data_json = json.dumps(payload).encode('utf-8')
                req_json = urlrequest.Request(endpoint, data=data_json, headers={'Content-Type': 'application/json', 'Accept': 'application/json', 'User-Agent': 'Smartly/1.0'})
                with urlrequest.urlopen(req_json, timeout=20) as resp:
                    body = resp.read().decode('utf-8')
                    parsed = json.loads(body)
                    # Expected: list of { language: 'en', confidence: 0.99 }
                    if isinstance(parsed, list) and parsed:
                        lang = parsed[0].get('language')
                        if lang:
                            return lang
            except Exception:
                pass
            try:
                data_form = urlencode(payload).encode('utf-8')
                req_form = urlrequest.Request(endpoint, data=data_form, headers={'Content-Type': 'application/x-www-form-urlencoded', 'Accept': 'application/json', 'User-Agent': 'Smartly/1.0'})
                with urlrequest.urlopen(req_form, timeout=20) as resp:
                    body = resp.read().decode('utf-8')
                    parsed = json.loads(body)
                    if isinstance(parsed, list) and parsed:
                        lang = parsed[0].get('language')
                        if lang:
                            return lang
            except Exception:
                continue
        return None

    def _translate_chunk(chunk):
        chunk = chunk or ''
        if not chunk.strip():
            return ''
        # Determine source language if auto was requested
        src = source_language_code or 'auto'
        if (src == 'auto'):
            detected = _detect_language(chunk)
            if detected:
                src = detected
        payload = {
            'q': chunk,
            'source': src,
            'target': target_language_code,
            'format': 'text'
        }
        for endpoint in endpoints:
            # Try JSON first
            try:
                data_json = json.dumps(payload).encode('utf-8')
                req_json = urlrequest.Request(
                    endpoint,
                    data=data_json,
                    headers={
                        'Content-Type': 'application/json',
                        'Accept': 'application/json',
                        'User-Agent': 'Smartly/1.0'
                    }
                )
                with urlrequest.urlopen(req_json, timeout=30) as resp:
                    body = resp.read().decode('utf-8')
                    parsed = json.loads(body)
                    translated = parsed.get('translatedText', '')
                    if translated and translated != chunk:
                        return translated
            except HTTPError as e:
                # Attempt to read error body to see if it contains JSON we can parse
                try:
                    err_body = e.read().decode('utf-8')
                    parsed_err = json.loads(err_body)
                    translated = parsed_err.get('translatedText', '')
                    if translated:
                        return translated
                except Exception:
                    pass
                # Fall through to form-encoded attempt
            except (URLError, Exception):
                # Fall through to form-encoded attempt
                pass

            # Form-encoded fallback
            try:
                data_form = urlencode(payload).encode('utf-8')
                req_form = urlrequest.Request(
                    endpoint,
                    data=data_form,
                    headers={
                        'Content-Type': 'application/x-www-form-urlencoded',
                        'Accept': 'application/json',
                        'User-Agent': 'Smartly/1.0'
                    }
                )
                with urlrequest.urlopen(req_form, timeout=30) as resp:
                    body = resp.read().decode('utf-8')
                    parsed = json.loads(body)
                    translated = parsed.get('translatedText', '')
                    if translated and translated != chunk:
                        return translated
            except Exception:
                # Try next endpoint
                continue

        # Secondary fallback: MyMemory Translated API (free)
        try:
            def _mm_code(code: str) -> str:
                c = (code or '').lower()
                # Normalize Chinese variants
                if c in ('zh', 'zh-cn', 'cn', 'zh-hans'):
                    return 'zh-CN'
                if c in ('zh-tw', 'tw', 'zh-hant', 'zh-hk', 'hk'):
                    return 'zh-TW'
                # MyMemory does NOT support 'auto' or empty sources; default to English
                if not c or c in ('auto', 'und', 'unknown'):
                    return 'en'
                # Return as-is for typical 2-letter codes
                return code

            def _mm_translate_small(text_part: str) -> str:
                src_pair = f"{_mm_code(payload['source'])}|{_mm_code(payload['target'])}"
                query = urlencode({'q': text_part, 'langpair': src_pair})
                url = f"https://api.mymemory.translated.net/get?{query}"
                req_mm = urlrequest.Request(url, headers={'Accept': 'application/json', 'User-Agent': 'Smartly/1.0'})
                with urlrequest.urlopen(req_mm, timeout=30) as resp:
                    body = resp.read().decode('utf-8')
                    parsed = json.loads(body)
                    translated = ''
                    if isinstance(parsed, dict):
                        translated = (parsed.get('responseData') or {}).get('translatedText', '')
                        if translated == text_part:
                            matches = parsed.get('matches') or []
                            for m in matches:
                                cand = m.get('translation')
                                if cand and cand != text_part:
                                    translated = cand
                                    break
                    return translated

            # MyMemory free API limits q to ~500 chars. Split into ~450-char chunks.
            mm_max = 450
            if len(chunk) <= mm_max:
                mm_out = _mm_translate_small(chunk)
                if mm_out and mm_out != chunk:
                    return mm_out
            else:
                out_parts = []
                start = 0
                while start < len(chunk):
                    end = min(start + mm_max, len(chunk))
                    newline_pos = chunk.rfind('\n', start, end)
                    space_pos = chunk.rfind(' ', start, end)
                    if newline_pos != -1 and newline_pos > start + 50:
                        end = newline_pos
                    elif space_pos != -1 and space_pos > start + 50:
                        end = space_pos
                    part = chunk[start:end]
                    translated_part = _mm_translate_small(part)
                    out_parts.append(translated_part or part)
                    start = end
                mm_joined = ''.join(out_parts)
                if mm_joined and mm_joined != chunk:
                    return mm_joined
        except Exception:
            pass

        # If none of the endpoints succeeded or translation unchanged
        return f"[Translation unchanged: provider unavailable or returned same text]"

    # Chunk by ~4000 characters to stay under typical API limits
    chunks = []
    max_len = 4000
    text = text or ''
    if len(text) <= max_len:
        chunks = [text]
    else:
        start = 0
        while start < len(text):
            end = min(start + max_len, len(text))
            # try to break at a newline for cleaner splits
            newline_pos = text.rfind('\n', start, end)
            if newline_pos != -1 and newline_pos > start + 1000:
                end = newline_pos
            chunks.append(text[start:end])
            start = end

    translated_parts = [_translate_chunk(c) for c in chunks]
    return ''.join(translated_parts)

def chat_with_openai(messages, system_prompt=None, model="gpt-3.5-turbo", max_tokens=800):
    """Generic chat helper using OpenAI ChatCompletion. Library uses this and must stay OpenAI."""
    try:
        final_messages = []
        if system_prompt:
            final_messages.append({"role": "system", "content": system_prompt})
        final_messages.extend(messages)
        response = openai.ChatCompletion.create(
            model=model,
            messages=final_messages,
            max_tokens=max_tokens,
            temperature=0.3,
        )
        return (response.choices[0].message.content or "") + f"\n\n[model: {model}]"
    except Exception as e:
        return f"Error chatting with OpenAI: {str(e)}\n\n[model: {model}]"


def recommend_youtube_videos_web(query, max_results=5, timeout=15, region=None):
    """Fetch current YouTube video recommendations using DuckDuckGo HTML search and YouTube oEmbed.
    - Returns a fenced smartly_videos JSON block for rich card rendering.
    - Filters out YouTube Shorts and deduplicates links.
    """
    try:
        q = f"site:youtube.com {query}".strip()
        ddg_url = "https://duckduckgo.com/html/?q=" + quote(q) + (f"&kl={quote(region)}" if region else "")
        req = urlrequest.Request(ddg_url, headers={'User-Agent': 'Smartly/1.0', 'Accept': 'text/html'})
        with urlrequest.urlopen(req, timeout=timeout) as resp:
            html = resp.read().decode('utf-8', errors='ignore')
    except Exception as e:
        return f"Error searching for videos: {str(e)}"

    # Extract result links from DuckDuckGo HTML (no-JS version)
    links = []
    try:
        for m in re.finditer(r'<a[^>]+class="[^\"]*result__a[^\"]*"[^>]+href="([^"]+)"', html, re.IGNORECASE):
            href = m.group(1)
            url = href
            if 'uddg=' in href:
                try:
                    parsed = urlparse(href)
                    qs = parse_qs(parsed.query)
                    if 'uddg' in qs and qs['uddg']:
                        url = unquote(qs['uddg'][0])
                    else:
                        mm = re.search(r'uddg=([^&]+)', href)
                        url = unquote(mm.group(1)) if mm else href
                except Exception:
                    url = href
            # Accept watch links and youtu.be; exclude shorts
            is_watch = ('youtube.com/watch' in url) or ('youtu.be/' in url)
            is_short = ('/shorts/' in url)
            if is_watch and not is_short:
                links.append(url)
            if len(links) >= max_results * 3:
                break
    except Exception:
        pass

    # Deduplicate while preserving order
    unique = []
    seen = set()
    for u in links:
        key = u.split('&')[0]
        if key not in seen:
            seen.add(key)
            unique.append(u)
        if len(unique) >= max_results * 2:
            break

    # Fetch title/channel via YouTube oEmbed (no API key) and build JSON
    results = []
    for link in unique:
        title = None
        channel = None
        thumb = None
        try:
            oembed = "https://www.youtube.com/oembed?format=json&url=" + quote(link, safe='')
            rq = urlrequest.Request(oembed, headers={'User-Agent': 'Smartly/1.0', 'Accept': 'application/json'})
            with urlrequest.urlopen(rq, timeout=timeout) as r2:
                data = json.loads(r2.read().decode('utf-8'))
                title = data.get('title')
                channel = data.get('author_name')
        except Exception:
            pass
        try:
            # Extract video id for thumbnail
            m = re.search(r'(?:v=|/)([a-zA-Z0-9_-]{11})(?:[&?/]|$)', link)
            vid_id = m.group(1) if m else None
            if vid_id:
                thumb = f"https://i.ytimg.com/vi/{vid_id}/hqdefault.jpg"
        except Exception:
            thumb = None
        results.append({'url': link, 'title': title or '', 'channel': channel or '', 'thumb': thumb or ''})

    if not results:
        return (
            "I couldn't find live YouTube links right now. Try refining the topic "
            "or check your network."
        )

    # Return structured fenced block for card rendering
    try:
        payload = json.dumps(results[:max_results])
        return f"```smartly_videos\n{payload}\n```"
    except Exception:
        # Fallback to markdown list
        lines = ["Here are current YouTube picks for your topic:", ""]
        for i, r in enumerate(results[:max_results], 1):
            t = r['title'] or f"Video {i}"
            ch = r['channel'] or 'YouTube'
            lines.append(f"- {t} — {ch}\n  {r['url']}")
        return "\n".join(lines)

    # Compose reply
    lines = ["Here are current YouTube picks for your topic:", ""]
    for i, r in enumerate(results, 1):
        t = r['title'] or f"Video {i}"
        ch = r['channel'] or 'YouTube'
        lines.append(f"- {t} — {ch}\n  {r['url']}")
    return "\n".join(lines)